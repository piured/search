{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load the dataset from a JSON file\n",
    "with open('../data/corpus/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "d = {\n",
    "    'slot': [item['slot'] for item in data],\n",
    "    'text': [item['text'] for item in data],\n",
    "    'position': [item['positions'] for item in data]\n",
    "}\n",
    "\n",
    "\n",
    "# create dataset from dict (train split)\n",
    "dataset = Dataset.from_dict(d)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes(data):\n",
    "    l = []\n",
    "    for item in data:\n",
    "        l.extend(item['slot'].keys())\n",
    "\n",
    "    list_set = set(l)\n",
    "    length = len(list_set)\n",
    "    return list(list_set), length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_labels, no_classes = classes(data)\n",
    "print(class_labels)\n",
    "print(no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_mapper(class_labels):\n",
    "    no_classes = len(class_labels)\n",
    "    d = {}\n",
    "    # 0 is reserved for no class\n",
    "    d['O'] = 0\n",
    "    for i in range(1, no_classes + 1):\n",
    "        d[class_labels[i-1]] = i\n",
    "    \n",
    "    # and another dictionary to map the index to the class label\n",
    "\n",
    "    d_reverse = {}\n",
    "    d_reverse[0] = 'O'\n",
    "    for i in range(1, no_classes + 1):\n",
    "        d_reverse[i] = class_labels[i-1]\n",
    "\n",
    "    return d, d_reverse\n",
    "\n",
    "mapper, unmapper = class_mapper(class_labels)\n",
    "print(mapper, unmapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_positions_with_tokens(input_ids, offset_mapping, position):\n",
    "    begin_end_tokens = [(offset_mapping[i][0], offset_mapping[i][1])  for i, token in enumerate(input_ids)]\n",
    "    labels = [ 0 for i in range(len(input_ids))]\n",
    "    labels[0] = -100\n",
    "    # find index of SEP token (102)\n",
    "    sep_index = input_ids.index(102)\n",
    "    # from sep_index to the end of the list, set the label to -100\n",
    "    for i in range(sep_index, len(labels)):\n",
    "        labels[i] = -100\n",
    "\n",
    "    \n",
    "    begin_end_tokens = begin_end_tokens[1:-1]\n",
    "    for key, val in position.items():\n",
    "        if (val != None):\n",
    "            begin_gt = val['begin']\n",
    "            end_gt = val['end']\n",
    "\n",
    "            class_label = key\n",
    "\n",
    "\n",
    "            # find the indices of the tokens that contain the begin and end of the ground truth\n",
    "            begin_token = [i for i, token in enumerate(begin_end_tokens) if token[0] >= begin_gt and token[0] < end_gt ]\n",
    "            end_token = [i for i, token in enumerate(begin_end_tokens) if token[1] == end_gt ]\n",
    "\n",
    "\n",
    "            # create list of indices of tokens that are part of the ground truth\n",
    "            try: \n",
    "                tokens = [i for i in range(begin_token[0], end_token[0]+1)]\n",
    "            except Exception as e:\n",
    "                print(begin_end_tokens)\n",
    "                print(f'begin: {begin_gt}, end: {end_gt}, class: {class_label}')\n",
    "                print(f'begin_token: {begin_token}, end_token: {end_token}')\n",
    "                print('---')\n",
    "                raise e\n",
    "\n",
    "            # set the label of the tokens that are part of the ground truth\n",
    "            for token in tokens:\n",
    "                labels[token + 1] = mapper[class_label]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "sample = dataset['train'][12239]\n",
    "inputs = tokenizer(sample[\"text\"], is_split_into_words=False, return_offsets_mapping=True)\n",
    "print(sample['text'])\n",
    "print(inputs.tokens())\n",
    "labels = align_positions_with_tokens(inputs['input_ids'], inputs['offset_mapping'], sample['position'])\n",
    "print(labels)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], is_split_into_words=False, return_offsets_mapping=True, truncation=True, padding=True, max_length=48\n",
    "    )\n",
    "    new_labels = []\n",
    "    for i, position in enumerate(examples['position']):\n",
    "        try:\n",
    "            new_labels.append(align_positions_with_tokens(tokenized_inputs['input_ids'][i], tokenized_inputs['offset_mapping'][i], position))\n",
    "        except Exception as e:\n",
    "            print( examples['text'][i])\n",
    "            print(tokenized_inputs.tokens(i))\n",
    "            print(f'index: {i}')\n",
    "            raise e\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"slot\", \"text\", \"position\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(labels):\n",
    "    return [unmapper[label] if label != -100 else 'O' for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoded_labels = decode_labels(tokenized_datasets['train'][\"labels\"][0])\n",
    "print(decoded_labels)\n",
    "pred = decoded_labels.copy()\n",
    "pred[1] = 'bpm_greater_than'\n",
    "\n",
    "metric.compute(predictions=[pred], references=[decoded_labels], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[ unmapper[label] for label in sample if label != -100] for sample in labels]\n",
    "    true_predictions = [\n",
    "        [unmapper[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForTokenClassification \n",
    "\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=unmapper,\n",
    "    label2id=mapper,\n",
    ")\n",
    "\n",
    "model.config.num_labels = no_classes + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"distilbert-piu-search\",\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from disk checkpoint\n",
    "model_cpu = DistilBertForTokenClassification.from_pretrained(\n",
    "    \"distilbert-piu-search/checkpoint-6094\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_cpu, tokenizer=tokenizer, aggregation_strategy=\"simple\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = token_classifier(\"EXC d 20\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ { \"class\": p['entity_group'], \"word\": p['word'], \"start\": p['start'], \"end\": p['end']} for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
