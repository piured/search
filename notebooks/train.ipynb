{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load the dataset from a JSON file\n",
    "with open('../data/corpus/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "d = {\n",
    "    'slot': [item['slot'] for item in data],\n",
    "    'text': [item['text'] for item in data],\n",
    "    'position': [item['positions'] for item in data]\n",
    "}\n",
    "\n",
    "\n",
    "# create dataset from dict (train split)\n",
    "dataset = Dataset.from_dict(d)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes(data):\n",
    "    l = []\n",
    "    for item in data:\n",
    "        l.extend(item['slot'].keys())\n",
    "\n",
    "    list_set = set(l)\n",
    "    length = len(list_set)\n",
    "    return list(list_set), length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artist', 'credit', 'bpm_lower_than', 'pump_routine', 'warps', 'fakes', 'pump_halfdouble', 'scrolls', 'pump_single', 'mix', 'stops', 'meter_greater_than', 'tune', 'pump_double', 'speeds', 'bpm_greater_than', 'meter_lower_than', 'pump_couple', 'meter', 'bpm']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_labels, no_classes = classes(data)\n",
    "print(class_labels)\n",
    "print(no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'artist': 1, 'credit': 2, 'bpm_lower_than': 3, 'pump_routine': 4, 'warps': 5, 'fakes': 6, 'pump_halfdouble': 7, 'scrolls': 8, 'pump_single': 9, 'mix': 10, 'stops': 11, 'meter_greater_than': 12, 'tune': 13, 'pump_double': 14, 'speeds': 15, 'bpm_greater_than': 16, 'meter_lower_than': 17, 'pump_couple': 18, 'meter': 19, 'bpm': 20} {0: 'O', 1: 'artist', 2: 'credit', 3: 'bpm_lower_than', 4: 'pump_routine', 5: 'warps', 6: 'fakes', 7: 'pump_halfdouble', 8: 'scrolls', 9: 'pump_single', 10: 'mix', 11: 'stops', 12: 'meter_greater_than', 13: 'tune', 14: 'pump_double', 15: 'speeds', 16: 'bpm_greater_than', 17: 'meter_lower_than', 18: 'pump_couple', 19: 'meter', 20: 'bpm'}\n"
     ]
    }
   ],
   "source": [
    "def class_mapper(class_labels):\n",
    "    no_classes = len(class_labels)\n",
    "    d = {}\n",
    "    # 0 is reserved for no class\n",
    "    d['O'] = 0\n",
    "    for i in range(1, no_classes + 1):\n",
    "        d[class_labels[i-1]] = i\n",
    "    \n",
    "    # and another dictionary to map the index to the class label\n",
    "\n",
    "    d_reverse = {}\n",
    "    d_reverse[0] = 'O'\n",
    "    for i in range(1, no_classes + 1):\n",
    "        d_reverse[i] = class_labels[i-1]\n",
    "\n",
    "    return d, d_reverse\n",
    "\n",
    "mapper, unmapper = class_mapper(class_labels)\n",
    "print(mapper, unmapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "album EXCEED~ZE\n",
      "['[CLS]', 'album', 'exceed', '~', 'ze', '[SEP]']\n",
      "[-100, 0, 10, 10, 10, -100]\n"
     ]
    }
   ],
   "source": [
    "def align_positions_with_tokens(input_ids, offset_mapping, position):\n",
    "    begin_end_tokens = [(offset_mapping[i][0], offset_mapping[i][1])  for i, token in enumerate(input_ids)]\n",
    "    labels = [ 0 for i in range(len(input_ids))]\n",
    "    labels[0] = -100\n",
    "    # find index of SEP token (102)\n",
    "    sep_index = input_ids.index(102)\n",
    "    # from sep_index to the end of the list, set the label to -100\n",
    "    for i in range(sep_index, len(labels)):\n",
    "        labels[i] = -100\n",
    "\n",
    "    \n",
    "    begin_end_tokens = begin_end_tokens[1:-1]\n",
    "    for key, val in position.items():\n",
    "        if (val != None):\n",
    "            begin_gt = val['begin']\n",
    "            end_gt = val['end']\n",
    "\n",
    "            class_label = key\n",
    "\n",
    "\n",
    "            # find the indices of the tokens that contain the begin and end of the ground truth\n",
    "            begin_token = [i for i, token in enumerate(begin_end_tokens) if token[0] >= begin_gt and token[0] < end_gt ]\n",
    "            end_token = [i for i, token in enumerate(begin_end_tokens) if token[1] == end_gt ]\n",
    "\n",
    "\n",
    "            # create list of indices of tokens that are part of the ground truth\n",
    "            try: \n",
    "                tokens = [i for i in range(begin_token[0], end_token[0]+1)]\n",
    "            except Exception as e:\n",
    "                print(begin_end_tokens)\n",
    "                print(f'begin: {begin_gt}, end: {end_gt}, class: {class_label}')\n",
    "                print(f'begin_token: {begin_token}, end_token: {end_token}')\n",
    "                print('---')\n",
    "                raise e\n",
    "\n",
    "            # set the label of the tokens that are part of the ground truth\n",
    "            for token in tokens:\n",
    "                labels[token + 1] = mapper[class_label]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "sample = dataset['train'][12239]\n",
    "inputs = tokenizer(sample[\"text\"], is_split_into_words=False, return_offsets_mapping=True)\n",
    "print(sample['text'])\n",
    "print(inputs.tokens())\n",
    "labels = align_positions_with_tokens(inputs['input_ids'], inputs['offset_mapping'], sample['position'])\n",
    "print(labels)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], is_split_into_words=False, return_offsets_mapping=True, truncation=True, padding=True, max_length=48\n",
    "    )\n",
    "    new_labels = []\n",
    "    for i, position in enumerate(examples['position']):\n",
    "        try:\n",
    "            new_labels.append(align_positions_with_tokens(tokenized_inputs['input_ids'][i], tokenized_inputs['offset_mapping'][i], position))\n",
    "        except Exception as e:\n",
    "            print( examples['text'][i])\n",
    "            print(tokenized_inputs.tokens(i))\n",
    "            print(f'index: {i}')\n",
    "            raise e\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 97500/97500 [00:07<00:00, 12942.10 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 14671.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"slot\", \"text\", \"position\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(labels):\n",
    "    return [unmapper[label] if label != -100 else 'O' for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'stops', 'stops', 'O', 'O', 'meter_lower_than', 'O', 'O', 'O', 'bpm_greater_than', 'tune', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: stops seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: meter_lower_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: bpm_greater_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: tune seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eter_lower_than': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'pm_greater_than': {'precision': 0.5,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 1},\n",
       " 'tops': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'une': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 0.6,\n",
       " 'overall_recall': 0.75,\n",
       " 'overall_f1': 0.6666666666666665,\n",
       " 'overall_accuracy': 0.9629629629629629}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decoded_labels = decode_labels(tokenized_datasets['train'][\"labels\"][0])\n",
    "print(decoded_labels)\n",
    "pred = decoded_labels.copy()\n",
    "pred[1] = 'bpm_greater_than'\n",
    "\n",
    "metric.compute(predictions=[pred], references=[decoded_labels], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[ unmapper[label] for label in sample if label != -100] for sample in labels]\n",
    "    true_predictions = [\n",
    "        [unmapper[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForTokenClassification \n",
    "\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=unmapper,\n",
    "    label2id=mapper,\n",
    ")\n",
    "\n",
    "model.config.num_labels = no_classes + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"distilbert-piu-search\",\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18282 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  3%|▎         | 500/18282 [00:14<07:59, 37.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4065, 'learning_rate': 1.9453013893447107e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: speeds seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_routine seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: meter_greater_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: bpm_lower_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: fakes seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: meter seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: credit seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: mix seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: bpm seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: scrolls seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_single seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_couple seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: artist seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_double seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: warps seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_halfdouble seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "                                                   \n",
      "  3%|▎         | 505/18282 [00:15<31:01,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05714792385697365, 'eval_precision': 0.9706809151938621, 'eval_recall': 0.976433296582139, 'eval_f1': 0.9735486087255237, 'eval_accuracy': 0.9834633092173259, 'eval_runtime': 1.4685, 'eval_samples_per_second': 1702.375, 'eval_steps_per_second': 213.137, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1000/18282 [00:30<08:08, 35.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0414, 'learning_rate': 1.8906027786894213e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  5%|▌         | 1005/18282 [00:31<30:46,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0379437692463398, 'eval_precision': 0.9803679297089511, 'eval_recall': 0.9841510474090408, 'eval_f1': 0.9822558459422285, 'eval_accuracy': 0.9887249835572677, 'eval_runtime': 1.5014, 'eval_samples_per_second': 1665.168, 'eval_steps_per_second': 208.479, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1500/18282 [00:44<07:12, 38.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0326, 'learning_rate': 1.8359041680341322e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  8%|▊         | 1507/18282 [00:46<28:23,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03233273699879646, 'eval_precision': 0.982147761603955, 'eval_recall': 0.9856670341786108, 'eval_f1': 0.983904250928601, 'eval_accuracy': 0.9901343606126092, 'eval_runtime': 1.4467, 'eval_samples_per_second': 1728.09, 'eval_steps_per_second': 216.357, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2000/18282 [00:59<06:51, 39.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0279, 'learning_rate': 1.7812055573788428e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 11%|█         | 2006/18282 [01:00<26:54, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.025209957733750343, 'eval_precision': 0.9848797250859107, 'eval_recall': 0.9874586549062845, 'eval_f1': 0.9861675039570573, 'eval_accuracy': 0.9916376961383069, 'eval_runtime': 1.456, 'eval_samples_per_second': 1716.99, 'eval_steps_per_second': 214.967, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 2500/18282 [01:14<06:57, 37.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0245, 'learning_rate': 1.7265069467235534e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 14%|█▎        | 2505/18282 [01:15<26:57,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.025493081659078598, 'eval_precision': 0.9854215376151836, 'eval_recall': 0.9874586549062845, 'eval_f1': 0.9864390445377573, 'eval_accuracy': 0.991731654608663, 'eval_runtime': 1.4382, 'eval_samples_per_second': 1738.32, 'eval_steps_per_second': 217.638, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 3000/18282 [01:29<07:15, 35.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0206, 'learning_rate': 1.671808336068264e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 16%|█▋        | 3005/18282 [01:30<27:40,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.021747741848230362, 'eval_precision': 0.9857142857142858, 'eval_recall': 0.9889746416758545, 'eval_f1': 0.9873417721518987, 'eval_accuracy': 0.9928591562529362, 'eval_runtime': 1.5272, 'eval_samples_per_second': 1636.943, 'eval_steps_per_second': 204.945, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3500/18282 [01:44<06:34, 37.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0201, 'learning_rate': 1.6171097254129746e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 19%|█▉        | 3505/18282 [01:46<26:40,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0211471114307642, 'eval_precision': 0.9887161139397276, 'eval_recall': 0.9902149944873209, 'eval_f1': 0.9894649865730221, 'eval_accuracy': 0.9934229070750729, 'eval_runtime': 1.5386, 'eval_samples_per_second': 1624.888, 'eval_steps_per_second': 203.436, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4000/18282 [02:00<06:27, 36.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0185, 'learning_rate': 1.562411114757685e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 22%|██▏       | 4005/18282 [02:01<25:29,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.018788378685712814, 'eval_precision': 0.989946288390029, 'eval_recall': 0.9906284454244763, 'eval_f1': 0.9902872494317008, 'eval_accuracy': 0.9942215540730996, 'eval_runtime': 1.5042, 'eval_samples_per_second': 1662.05, 'eval_steps_per_second': 208.089, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 4500/18282 [02:15<06:07, 37.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0171, 'learning_rate': 1.5077125041023959e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|██▍       | 4505/18282 [02:17<25:15,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.016791068017482758, 'eval_precision': 0.9920165175498967, 'eval_recall': 0.9932469680264608, 'eval_f1': 0.9926313614764823, 'eval_accuracy': 0.9953490557173729, 'eval_runtime': 1.5678, 'eval_samples_per_second': 1594.578, 'eval_steps_per_second': 199.641, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 5000/18282 [02:31<05:50, 37.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0153, 'learning_rate': 1.4530138934471065e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 27%|██▋       | 5005/18282 [02:32<23:57,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01705148071050644, 'eval_precision': 0.9929752066115702, 'eval_recall': 0.9935226019845645, 'eval_f1': 0.9932488288784789, 'eval_accuracy': 0.995396034952551, 'eval_runtime': 1.5385, 'eval_samples_per_second': 1624.914, 'eval_steps_per_second': 203.439, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 5500/18282 [02:46<05:34, 38.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.015, 'learning_rate': 1.3983152827918172e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|███       | 5505/18282 [02:48<22:46,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012150906957685947, 'eval_precision': 0.9936665289825141, 'eval_recall': 0.994625137816979, 'eval_f1': 0.9941456023142088, 'eval_accuracy': 0.9967114535375364, 'eval_runtime': 1.5147, 'eval_samples_per_second': 1650.456, 'eval_steps_per_second': 206.637, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6000/18282 [03:02<05:25, 37.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0149, 'learning_rate': 1.3436166721365278e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|███▎      | 6005/18282 [03:04<23:50,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011320582590997219, 'eval_precision': 0.9924304982108451, 'eval_recall': 0.9937982359426681, 'eval_f1': 0.9931138961575541, 'eval_accuracy': 0.9959597857746876, 'eval_runtime': 1.702, 'eval_samples_per_second': 1468.899, 'eval_steps_per_second': 183.906, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 6500/18282 [03:18<05:15, 37.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0097, 'learning_rate': 1.2889180614812384e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: speeds seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_routine seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: meter_greater_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: meter_lower_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: bpm_lower_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: fakes seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: meter seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: credit seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: mix seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: bpm seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: scrolls seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_single seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: stops seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_couple seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: artist seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_double seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: bpm_greater_than seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: tune seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: warps seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pedro/opensource/piured-search/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: pump_halfdouble seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "                                                    \n",
      " 36%|███▌      | 6505/18282 [03:20<21:31,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012142508290708065, 'eval_precision': 0.9946280991735538, 'eval_recall': 0.9951764057331863, 'eval_f1': 0.9949021769082392, 'eval_accuracy': 0.9967584327727145, 'eval_runtime': 1.5616, 'eval_samples_per_second': 1600.903, 'eval_steps_per_second': 200.433, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6913/18282 [03:31<05:03, 37.47it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/opensource/piured-search/.venv/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/opensource/piured-search/.venv/lib/python3.11/site-packages/transformers/trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1815\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1816\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/opensource/piured-search/.venv/lib/python3.11/site-packages/accelerate/data_loader.py:393\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    394\u001b[0m     next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n",
      "File \u001b[0;32m~/opensource/piured-search/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 160\u001b[0m         {\n\u001b[1;32m    161\u001b[0m             k: t \u001b[39mif\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m skip_keys \u001b[39melse\u001b[39;49;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;49;00m k, t \u001b[39min\u001b[39;49;00m tensor\u001b[39m.\u001b[39;49mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opensource/piured-search/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m    160\u001b[0m         {\n\u001b[0;32m--> 161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opensource/piured-search/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[1;32m    168\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from disk checkpoint\n",
    "model_cpu = DistilBertForTokenClassification.from_pretrained(\n",
    "    \"distilbert-piu-search/checkpoint-6094\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_cpu, tokenizer=tokenizer, aggregation_strategy=\"simple\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'mix', 'score': 0.9676077, 'word': 'exc', 'start': 0, 'end': 3}, {'entity_group': 'pump_double', 'score': 0.9950138, 'word': 'd', 'start': 4, 'end': 5}, {'entity_group': 'meter', 'score': 0.9997477, 'word': '20', 'start': 6, 'end': 8}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6914/18282 [03:45<05:03, 37.47it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = token_classifier(\"EXC d 20\", )\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ { \"class\": p['entity_group'], \"word\": p['word'], \"start\": p['start'], \"end\": p['end']} for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
